{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import networkx as nx\n",
    "import community\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import json\n",
    "import inspect\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to DB\n",
    "# follows django database settings format\n",
    "DATABASES = {'production':{'NAME': '',\n",
    "                           'USER': '',\n",
    "                           'PASSWORD': '',\n",
    "                           'HOST': '',\n",
    "                           'PORT': ,\n",
    "                          },}\n",
    "\n",
    "# choose the database to use\n",
    "db = DATABASES['production']\n",
    "\n",
    "# construct an engine connection string\n",
    "engine_string = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(user = db['USER'],\n",
    "                                                                                          password = db['PASSWORD'],\n",
    "                                                                                          host = db['HOST'],\n",
    "                                                                                          port = db['PORT'],\n",
    "                                                                                          database = db['NAME'],)\n",
    "\n",
    "# create sqlalchemy engine\n",
    "engine = create_engine(engine_string)\n",
    "\n",
    "metadata = None\n",
    "try:\n",
    "    metadata = MetaData(bind=engine, reflect=True)\n",
    "except:\n",
    "    print('cant read metadata from DB')\n",
    "\n",
    "data_path = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SchemaDiscoveryDTO stores a list of tables in the input connection (tables of type Table) and list of dependencies between \n",
    "# the tables\n",
    "class SchemaDiscoveryDTO:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.tables = []\n",
    "        self.dependencies = None\n",
    "        \n",
    "# TableDTO contains a name, schema name (e.g. public), list of columns and list of entities (each entity is list of columns)\n",
    "class TableDTO:\n",
    "    def __init__(self, table_name):       \n",
    "        self.name = table_name\n",
    "        self.columns = []  \n",
    "        self.entities = None\n",
    "        self.size = None\n",
    "        \n",
    "# A column contains column name, raw_type(int, float, object) and type(label/free text/ numeric/ timestamp/ identifier code)\n",
    "class ColumnDTO:\n",
    "    def __init__(self, col_name, col_raw_type, col_type, is_pk, pk_source, is_fk, fk_source):\n",
    "        self.name = col_name\n",
    "        self.rawType = col_raw_type\n",
    "        self.type = col_type\n",
    "        self.isPK = is_pk\n",
    "        self.PKsource = pk_source\n",
    "        self.isFK = is_fk\n",
    "        self.FKsource = fk_source\n",
    "        \n",
    "        \n",
    "# A Entity contains a list of columns \n",
    "class EntityDTO:\n",
    "    def __init__(self):\n",
    "        self.columns = []\n",
    "        \n",
    "# TableRefDTO contain name of the table, name of the column, its cardinality and its relationship\n",
    "class TableRefDTO:\n",
    "    def __init__(self, table_name, column_name, cardinality_type, relationship_type):\n",
    "        self.tableName = table_name\n",
    "        self.columnName = column_name\n",
    "        self.cardinalityType = cardinality_type\n",
    "        self.relationshipType = relationship_type\n",
    "\n",
    "# DependencyDTO contain two TableRefDTO; left and right that represent the dependency between two columns and the source of the dependency (from metadata or founde by us)\n",
    "class DependencyDTO:\n",
    "    def __init__(self, table_left, table_right, dependency_source):\n",
    "        self.left = table_left\n",
    "        self.right = table_right\n",
    "        self.dependencySource = dependency_source\n",
    "        \n",
    "#Serealize result to json\n",
    "class ObjectEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if hasattr(obj, \"to_json\"):\n",
    "            return self.default(obj.to_json())\n",
    "        elif hasattr(obj, \"__dict__\"):\n",
    "            d = dict(\n",
    "                (key, value)\n",
    "                for key, value in inspect.getmembers(obj)\n",
    "                if not key.startswith(\"__\")\n",
    "                and not inspect.isabstract(value)\n",
    "                and not inspect.isbuiltin(value)\n",
    "                and not inspect.isfunction(value)\n",
    "                and not inspect.isgenerator(value)\n",
    "                and not inspect.isgeneratorfunction(value)\n",
    "                and not inspect.ismethod(value)\n",
    "                and not inspect.ismethoddescriptor(value)\n",
    "                and not inspect.isroutine(value)\n",
    "            )\n",
    "            return self.default(d)\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data with given table.name as csv file and sample sample_size records if to_sample == True\n",
    "def get_data(data_path, table, sample_size, to_sample=True):\n",
    "    data = pd.read_csv(data_path + 'data/' + table.name + '.csv')\n",
    "\n",
    "    if to_sample:\n",
    "        how_many_take = min(table.size, sample_size)\n",
    "        sampled_records = data.sample(n=how_many_take)\n",
    "\n",
    "        return sampled_records\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Save schema_discovery object in given location\n",
    "def save_schema_discovery(schema_discovery, schema_discovery_file_location):\n",
    "    with open(schema_discovery_file_location, 'w', encoding='utf-8') as f:\n",
    "        json.dump(schema_discovery, cls=ObjectEncoder, indent=2, fp=f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_data(table_name, schema_discovery, engine, path_to_save_data):\n",
    "    data = pd.read_sql_table(table_name, engine)\n",
    "    if data.shape[0] > 1:\n",
    "        data.to_csv(path_to_save_data, index=False)\n",
    "        table = TableDTO(table_name)\n",
    "        table.size = data.shape[0]\n",
    "        schema_discovery.tables.append(table)\n",
    "  \n",
    "  \n",
    "def get_schema_discovery_from_DB(engine, schema_name, data_path):  \n",
    "    tables_names = engine.table_names() \n",
    "    schema_discovery = SchemaDiscoveryDTO(schema_name)\n",
    "    threads_list = []\n",
    "    tables_dict = {}\n",
    "    for table_name in tables_names:\n",
    "        path_to_save_data = data_path + 'data/' + table_name + '.csv'\n",
    "        read_table_data(table_name, schema_discovery, engine, path_to_save_data)\n",
    "\n",
    "    return schema_discovery\n",
    "\n",
    "schema_discovery = get_schema_discovery_from_DB(engine, db['NAME'], data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An auxiliary function that accepts basic columns_data (name & raw data type per column) and a sample of records (based on these columns) and calculates the final column type (label/free text/ numeric/ timestamp/ identifier code)\n",
    "def calc_col_types(columns_data, sampled_records, table_metadata=None):   \n",
    "    # Calculates data types from sampled_records and updating columns_data accordingly\n",
    "    sample_size = sampled_records.shape[0] # the maximal number of unique values\n",
    "    long_str = 200 # A column containing strings with more than this number of characters will be considered free text column\n",
    "    label_threshold = 0.2 # labels are expected to contain unique values up to this percent of the number of non empty values\n",
    "    text_threshold = 0.8 # Free texts are expected to contain unique values of at least this percent of the number of non empty values\n",
    "    idenifier_threshold = 4 # Free texts are expected to contain at least this number of digits\n",
    "\n",
    "    res = columns_data.copy()\n",
    "    res['raw_type'] = res['raw_type'].astype(str)\n",
    "\n",
    "    # Add number of unique values per column\n",
    "    res['unique_vals'] = sampled_records.nunique().values\n",
    "\n",
    "    # Add number of non-null values per column\n",
    "    res['not_null_cnt'] = sampled_records.count(axis=0).values\n",
    "\n",
    "    res['min_digits'] = 0 \n",
    "    res['max_digits'] = 0 \n",
    "    res['is_bool'] = [False] * len(res)\n",
    "    res['is_str'] = [False] * len(res)\n",
    "    for col in sampled_records.columns:\n",
    "        col_vals_as_str = sampled_records[col].dropna().astype(str)\n",
    "        # Add minimal number of characters\n",
    "        min_str_len = col_vals_as_str.str.len().min()\n",
    "        res.loc[res['col_name']==col, 'min_digits'] = min_str_len\n",
    "\n",
    "        # Add maximal number of characters\n",
    "        max_str_len = col_vals_as_str.str.len().max()\n",
    "        res.loc[res['col_name']==col, 'max_digits'] = max_str_len\n",
    "\n",
    "        if ('True' in col_vals_as_str.values) | ('False' in col_vals_as_str.values):\n",
    "            res.loc[res['col_name']==col, 'is_bool'] = True\n",
    "\n",
    "        if not any(any(sub_str.isdigit() for sub_str in main_str) for main_str in col_vals_as_str.values):\n",
    "            res.loc[res['col_name']==col, 'is_str'] = True\n",
    "\n",
    "    res['col_type'] = res['raw_type'].copy()\n",
    "\n",
    "    # Checks if the column is numeric\n",
    "    is_numeric = np.vectorize(lambda x: True if any(sub_str in str(x) for sub_str in ['int', 'float']) else False)\n",
    "    res['is_numeric'] = is_numeric(res['raw_type']) \n",
    "\n",
    "    # Checks if the column is int\n",
    "    is_int = np.vectorize(lambda x: True if 'int' in str(x) else False)\n",
    "    res['is_int'] = is_int(res['raw_type'])\n",
    "\n",
    "    # Recognizing identifing keys as unique numbers or codes\n",
    "    msk_id_code = ((res['is_str']==False) & (res['is_bool']==False) & (res['min_digits'] == res['max_digits']) &\\\n",
    "                   (res['min_digits']>=idenifier_threshold) & ((res['is_int']==True) | (res['raw_type'] == 'object')))\n",
    "    res.loc[msk_id_code,'col_type'] = 'identifier code' \n",
    "\n",
    "    # Recognizing labels as repeating short texts\n",
    "    msk_lable = ((res['is_bool']==False) & (res['col_type'] == res['raw_type']) & (res['unique_vals']<res['not_null_cnt']*label_threshold) &\\\n",
    "                 (res['raw_type'] == 'object'))\n",
    "    res.loc[msk_lable,'col_type'] = 'label' \n",
    "\n",
    "    # Recognizing numeric columns as numeric columns that are not identifiers\n",
    "    msk_numeric = (res['is_bool']==False) & (res['col_type'] == res['raw_type']) & (res['is_numeric'])\n",
    "    res.loc[msk_numeric,'col_type'] = 'numeric'\n",
    "\n",
    "    # Recognizing free texts as unique or long texts\n",
    "    msk_free_text = ((res['is_bool']==False) & (res['col_type'] == res['raw_type']) & \\\n",
    "                     ((res['is_str']==True) | (((res['unique_vals']>=res['not_null_cnt']*text_threshold) | (res['max_digits']>=long_str)) & \\\n",
    "                                               (res['raw_type'] == 'object'))))\n",
    "    res.loc[msk_free_text,'col_type'] = 'free text' \n",
    "\n",
    "    # Recognizing bool columns as bool columns\n",
    "    msk_bool = (res['is_bool'])\n",
    "    res.loc[msk_bool,'col_type'] = 'bool'\n",
    "    \n",
    "    # Recognizing dates\n",
    "    msk_date = (res['raw_type'].str.contains('time'))\n",
    "    res.loc[msk_date,'col_type'] = 'timestamp'\n",
    "    \n",
    "    # Recognizing Primery Key\n",
    "    res['is_pk'] = False\n",
    "    res['pk_source'] = 'None'\n",
    "    if table_metadata != None:\n",
    "        for col in [col for col in table_metadata.columns if ((col.name in columns_data['col_name'].values) & (col.primary_key))]:\n",
    "            res.loc[res['col_name'] == col.name, 'is_pk'] = True\n",
    "            res.loc[res['col_name'] == col.name, 'pk_source'] = 'Metadata'\n",
    "    \n",
    "    else:\n",
    "        msk_pk = ((res['col_type']=='identifier code') & (res['unique_vals'] == res['not_null_cnt']) & (res['unique_vals'] == sample_size))\n",
    "        res.loc[msk_pk, 'is_pk'] = True \n",
    "        res.loc[msk_pk, 'pk_source'] = 'Discovered' \n",
    "\n",
    "        num_pk = res.loc[res['is_pk'] == True].shape[0]\n",
    "\n",
    "        # If no PK, search for combinations of identifier code columns that create x unqiue keys where x is the size of the data\n",
    "        if num_pk == 0:\n",
    "            id_code_cols = res.loc[(res['col_type'] == 'identifier code') & (res['not_null_cnt'] == sample_size)]['col_name'].values\n",
    "            for pk_len in range(2, len(id_code_cols)+1):\n",
    "                all_possible_combination = itertools.combinations(id_code_cols, pk_len)\n",
    "                for pk_group in all_possible_combination:\n",
    "                    num_unqiue_vals = len(sampled_records.groupby(list(pk_group)).groups)\n",
    "                    if num_unqiue_vals == sample_size:\n",
    "                        res.loc[res['col_name'].isin(pk_group), 'is_pk'] = True\n",
    "                        res.loc[res['col_name'].isin(pk_group), 'pk_source'] = 'Discovered'\n",
    "                        pk_len = len(id_code_cols)+1\n",
    "                        break \n",
    "                        \n",
    "        # If more then one column acts as PK (by itself, not as combination of columns) we need to choose only one of \n",
    "        # them as PK and the others will be regular identifier code columns (there is no additional information in keeping all as PK)\n",
    "        elif num_pk > 1:\n",
    "            all_pk_cols = res.loc[res['is_pk'] == True]['col_name']\n",
    "            res.loc[res['col_name'].isin(all_pk_cols[1:]), 'is_pk'] = False\n",
    "            res.loc[res['col_name'].isin(all_pk_cols[1:]), 'pk_source'] = 'None'\n",
    "    \n",
    "    return res\n",
    "\n",
    "# Extracting the list of columns with column names and their raw data types from table with given table.table_name\n",
    "def get_columns(table, data_path, sample_size, table_metadata=None):  \n",
    "    sampled_records = get_data(data_path, table, sample_size)\n",
    "  \n",
    "    cols_df = pd.DataFrame({'col_name':sampled_records.dtypes.index.values, 'raw_type':sampled_records.dtypes.values})\n",
    "    cols_data = calc_col_types(cols_df, sampled_records, table_metadata)\n",
    "  \n",
    "    table.columns = []\n",
    "    for idx, col in cols_data.iterrows():\n",
    "        # filter free text columns        \n",
    "        if str(col['col_type']) != 'free text':\n",
    "            table.columns.append(ColumnDTO(col['col_name'], str(col['raw_type']), str(col['col_type']), col['is_pk'], col['pk_source'], False, 'None'))\n",
    "        \n",
    "def get_tables_cols(schema_discovery, data_path, sample_size, metadata=None):    \n",
    "    threads_list = []\n",
    "    for table in schema_discovery.tables:\n",
    "        if metadata != None:\n",
    "            table_metadata = metadata.tables[table.name]\n",
    "        else:\n",
    "            table_metadata = None \n",
    "        get_columns(table, data_path, sample_size, table_metadata)\n",
    "        \n",
    "get_tables_cols(schema_discovery, data_path, 1000, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform all columns values to categorical values (e.g. from ['israel', 'usa', 'israel', 'spain'] to [1, 2, 1, 3])\n",
    "def to_categorical(data, columns_names):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    categorical_cols = {}\n",
    "    for col_name in columns_names:\n",
    "        col_real_vals = data[col_name].values\n",
    "        col_real_vals = ['None' if val is None else val for val in col_real_vals]\n",
    "        categorical_cols[col_name] = le.fit_transform(col_real_vals)\n",
    "    return categorical_cols\n",
    "\n",
    "# Calculate correlation matrix; extracts internal dependencies between column pairs that appear in data\n",
    "# Matrix corr_matrix contain the dependencies estimation between col_1 and col_2 (where col_1 != col_2) using mutual information measure\n",
    "def get_correlation_matrix(table, data): \n",
    "    data_col_names = [col.name for col in table.columns]\n",
    "    categorical_cols = to_categorical(data, data_col_names)\n",
    "    all_possible_cols_combinations = [x for x in itertools.combinations(data_col_names, 2)]\n",
    "    \n",
    "    corr_matrix = pd.DataFrame(columns=data_col_names, \n",
    "                               index=data_col_names, \n",
    "                               data=np.zeros((len(data_col_names), len(data_col_names))))\n",
    "    \n",
    "    for col_tuple in all_possible_cols_combinations:\n",
    "        col_1 = col_tuple[0]\n",
    "        col_2 = col_tuple[1]\n",
    "        \n",
    "        cat_col_1 = categorical_cols[col_1]\n",
    "        cat_col_2 = categorical_cols[col_2]\n",
    "        \n",
    "        # Calculate the Information Gain of target columns given the source column\n",
    "        mutulal_info = metrics.normalized_mutual_info_score(cat_col_1, cat_col_2)\n",
    "        corr_matrix.loc[col_1, col_2] = mutulal_info\n",
    "        corr_matrix.loc[col_2, col_1] = mutulal_info\n",
    "\n",
    "#     # plot heatmap of correlation matrix\n",
    "#     sns.heatmap(corr_matrix, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns, annot=True)\n",
    "\n",
    "    return corr_matrix\n",
    "    \n",
    "    \n",
    "# Create network graph using networkx package based on correlation matrix created by get_correlation_matrix function on given data\n",
    "# when filter_col is True -> correlation value is set to 0 for columns tuple with correlation value lower then mean correlation value \n",
    "def get_network_graph(corr_matrix, filter_col=True):\n",
    "    links = corr_matrix.stack().reset_index()\n",
    "    links.columns = ['var1', 'var2','value']\n",
    "\n",
    "    # Remove self correlation\n",
    "    links_filtered = links.loc[links['var1'] != links['var2']]\n",
    "\n",
    "    if filter_col:\n",
    "        # Keep only correlation over a threshold (the mean correlation value)\n",
    "        mean_corr = links_filtered.loc[links_filtered['value'] > 0]['value'].mean()\n",
    "        links_filtered = links_filtered.loc[links_filtered['value'] > mean_corr]\n",
    "        \n",
    "    # Build the graph\n",
    "    G = nx.from_pandas_edgelist(links_filtered, 'var1', 'var2', edge_attr='value')\n",
    "\n",
    "#     # Plot the network:\n",
    "#     nx.draw_circular(G, with_labels=True, node_size=200, font_size=10)\n",
    "\n",
    "    return G\n",
    "\n",
    "# Detects communities in the graph\n",
    "# The communities detection method we use can be one of the next communities detection methods:\n",
    "# 1. GN - girvan_newman \n",
    "# 2. best- best_partition (based on Louvain algorithm)\n",
    "def detect_communities(G, community_method='best'):\n",
    "    if community_method == 'GN':\n",
    "        comp = girvan_newman(G)\n",
    "        for communities in itertools.islice(comp, int(0.3*len(G.nodes))):\n",
    "            clusters = tuple(sorted(c) for c in communities)\n",
    "\n",
    "    elif community_method == 'best':\n",
    "        communities = community.best_partition(G)\n",
    "\n",
    "        clusters = []\n",
    "        cluster_dict = {}\n",
    "        for i in range(len(set([val for val in communities.values()]))):\n",
    "            curr_cluster = [key for key, val in communities.items() if val == i]\n",
    "            clusters.insert(i, curr_cluster)\n",
    "            for node in curr_cluster:\n",
    "                cluster_dict[node] = i\n",
    "                \n",
    "        nx.set_node_attributes(G, cluster_dict, 'cluster')\n",
    "     \n",
    "    return clusters\n",
    "\n",
    "# Plot the entities we found as communities on network (features) graph\n",
    "def plot_communty_network(G, path_to_save_plot, with_labels=True):\n",
    "    plt.figure(figsize=(8,8))\n",
    "    pos = nx.spring_layout(G, k=2)\n",
    "    node_colors = ['green', 'red', 'yellow', 'black', 'blue', 'orange', 'pink', 'purple', 'gray', 'brown']\n",
    "    \n",
    "#     edge_labels = nx.get_edge_attributes(G, 'value')\n",
    "#     for key_tuple in edge_labels:\n",
    "#       edge_labels[key_tuple] = round(edge_labels[key_tuple], 3)\n",
    "\n",
    "#     nx.draw_networkx_edges(G, pos, width=1, alpha=0.9)\n",
    "#     nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n",
    "   \n",
    "    edges = G.edges()\n",
    "    weights = [G[u][v]['value']*2 for u,v in edges]\n",
    "    nx.draw_networkx_edges(G, pos, width=weights, alpha=0.9)\n",
    "    \n",
    "    for node, node_att_dict in G.nodes(data=True):\n",
    "        cluster_idx = node_att_dict['cluster']\n",
    "        color = node_colors[cluster_idx%len(node_colors)]\n",
    "        nx.draw_networkx_nodes(G, pos, nodelist=[node], node_color=color, node_size=150)\n",
    "        if with_labels:\n",
    "            nx.draw_networkx_labels(G, pos, {node:node}, font_size=8)\n",
    "\n",
    "    nx.write_gexf(G, path_to_save_plot + '.gexf')\n",
    "    plt.savefig(path_to_save_plot)  \n",
    "    plt.show()\n",
    "\n",
    "# Main function that uses all previous functions to detect main entities for each table in schema_discovery\n",
    "def find_entities(schema_discovery, data_path, sample_size=10000):\n",
    "    threads_list = []\n",
    "    results_dict = {}\n",
    "    for table in schema_discovery.tables:\n",
    "        sampled_records = get_data(data_path, table, sample_size)\n",
    "        \n",
    "        corr_matrix = get_correlation_matrix(table, sampled_records)\n",
    "        G = get_network_graph(corr_matrix)\n",
    "        clusters = detect_communities(G)\n",
    "        plot_communty_network(G, data_path + 'plots/' + table.name)\n",
    "        print('finish finding clusters for table ' + table.name + ', num of clusters: ' + str(len(clusters)))\n",
    "        \n",
    "        table.entities = []\n",
    "        for cluster in clusters:\n",
    "            entity = EntityDTO()\n",
    "            entity.columns = cluster\n",
    "            table.entities.append(entity)\n",
    "            \n",
    "    return G\n",
    "            \n",
    "G = find_entities(schema_discovery, data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatinate number of columns to one unique PK column - done for compersion between PK of tables that build from number\n",
    "# of columns\n",
    "def combain_pk_columns(data, pk_list):\n",
    "    data['concat_pk_col'] = data[[col.name for col in pk_list]].apply(lambda row: '_'.join(row.values.astype(str)), axis=1)\n",
    "    pk_raw_types = [col.rawType for col in pk_list]\n",
    "    pk_raw_types.sort()  \n",
    "    new_pk_list = [ColumnDTO(name='concat_pk_col', \n",
    "                             rawType='_'.join(pk_raw_types), \n",
    "                             type='identifier code',\n",
    "                             isPK=True,\n",
    "                             PKsource='Discovered',\n",
    "                             isFK=False,\n",
    "                             FKsource='None')]\n",
    "    \n",
    "    return data, new_pk_list\n",
    "\n",
    "# Get the cardinality of column given the data. cardinality can get one of the next values:\n",
    "# '1': each unique value of this column appear only once\n",
    "# 'M': there is at least one unqiue value of this column that appear more then one time\n",
    "def get_cardinality(data, col_name):\n",
    "    map_cardinalty = lambda x : 'One' if (x == 1) else 'Many' \n",
    "    \n",
    "    cardinality = data.dropna(subset=[col_name]).groupby(col_name).count().max().max()\n",
    "    cardinality = map_cardinalty(cardinality)\n",
    "    \n",
    "    return cardinality\n",
    "\n",
    "# Search table object with given table_name in schema_discovery.tables_list. If there is no table with this name- return None\n",
    "def get_table_by_name(schema_discovery, table_name):\n",
    "    for table in schema_discovery.tables:\n",
    "        if table.name == table_name:\n",
    "            return table\n",
    "    return None\n",
    "\n",
    "# Change the type of each column that is a foreign key to 'FK' type\n",
    "def set_col_keytype_to_FK(table, col_name, FK_source):\n",
    "    for col in table.columns:\n",
    "        if (col.name == col_name) & (col.isPK == False) & (col.isFK == False):\n",
    "            col.isFK = True\n",
    "            col.FKsource = FK_source\n",
    "\n",
    "# Get the relationship between two columns; Contained, Contains, Partial Overlap, Equel (same values), or None if there is\n",
    "# no relationship between this two columns\n",
    "def get_relationship_type(data_1, data_2, col_1, col_2):\n",
    "    col_name_1 = col_1 + '_1'\n",
    "    col_name_2 = col_2 + '_2'                \n",
    "\n",
    "    vals_col_1 = pd.DataFrame(data=data_1[col_1].dropna().unique(), columns=[col_name_1])\n",
    "    vals_col_2 = pd.DataFrame(data=data_2[col_2].dropna().unique(), columns=[col_name_2])\n",
    "\n",
    "    col_1_relationship = None\n",
    "    col_2_relationship = None\n",
    "\n",
    "    join_on_cols = pd.merge(vals_col_1, vals_col_2, left_on=col_name_1, right_on=col_name_2, how='inner')\n",
    "\n",
    "    if (join_on_cols.shape[0] == len(vals_col_1)) & (join_on_cols.shape[0] == len(vals_col_2)):\n",
    "        col_1_relationship = 'Equel'\n",
    "        col_2_relationship = 'Equel'\n",
    "    else:\n",
    "        if join_on_cols.shape[0] == len(vals_col_1):\n",
    "            col_1_relationship = 'Contained'\n",
    "            col_2_relationship = 'Contains'\n",
    "        elif join_on_cols.shape[0] == len(vals_col_2):\n",
    "            col_1_relationship = 'Contains'\n",
    "            col_2_relationship = 'Contained'\n",
    "        elif join_on_cols.shape[0] > 0:\n",
    "            col_1_relationship = 'Overlap'\n",
    "            col_2_relationship = 'Overlap'\n",
    "            \n",
    "    return col_1_relationship, col_2_relationship\n",
    "            \n",
    "# Extracts external dependencies between tabels pairs\n",
    "# dataframe connected_tabels contains names of the two connected attributes, their tabels names, and the type of \n",
    "# dependency (1:1, 1:M, M:1, M:N)\n",
    "def find_foreign_keys(table_1, table_2, data_path):  \n",
    "    tables_dependencies = []        \n",
    "    data_1 = get_data(data_path, table_1, 0, to_sample=False)\n",
    "    data_2 = get_data(data_path, table_2, 0, to_sample=False)\n",
    "\n",
    "    pk_table_1 = [col for col in table_1.columns if col.isPK]\n",
    "    pk_table_2 = [col for col in table_2.columns if col.isPK]\n",
    "    identifier_code_cols_1 = [col for col in table_1.columns if ((col.type == 'identifier code') & (not col.isPK))]\n",
    "    identifier_code_cols_2 = [col for col in table_2.columns if ((col.type == 'identifier code') & (not col.isPK))]\n",
    "    \n",
    "    if (len(pk_table_1) == 0) & (len(pk_table_2) == 0):\n",
    "        return tables_dependencies\n",
    "    \n",
    "    # If the tables have diffrent size of PK we cant comper bwtween them -> check dependencies only between PK of size 1 and \n",
    "    # identifier code columns\n",
    "    if (len(pk_table_1) != len(pk_table_2)):\n",
    "        if (len(pk_table_1) > 1) & (len(pk_table_2) > 1):\n",
    "            return tables_dependencies\n",
    "        elif len(pk_table_1) > 1:\n",
    "            pk_table_1 = []\n",
    "        elif len(pk_table_2) > 1:\n",
    "            pk_table_2 = []\n",
    "    \n",
    "    # If the tables have same size of PK then we can comper them- first we concatinate all pk columns to one unique column\n",
    "    if (len(pk_table_1) == len(pk_table_2)) & (len(pk_table_1) > 1):\n",
    "        data_1, pk_table_1 = combain_pk_columns(data_1, pk_table_1)\n",
    "        data_2, pk_table_2 = combain_pk_columns(data_2, pk_table_2)    \n",
    "\n",
    "    pk_identifier_code_cols_1 = pk_table_1 + identifier_code_cols_1\n",
    "    pk_identifier_code_cols_2 = pk_table_2 + identifier_code_cols_2\n",
    "    \n",
    "    for col_1 in pk_identifier_code_cols_1:\n",
    "        for col_2 in pk_identifier_code_cols_2:\n",
    "            if (not col_1.isPK) & (not col_2.isPK):\n",
    "                continue\n",
    "            if col_1.rawType == col_2.rawType:\n",
    "                col_1_relationship, col_2_relationship = get_relationship_type(data_1, data_2, col_1.name, col_2.name)\n",
    "                        \n",
    "                if col_1_relationship != None:                    \n",
    "                    cardinality_col_1 = get_cardinality(data_1, col_1.name)\n",
    "                    cardinality_col_2 = get_cardinality(data_2, col_2.name)\n",
    "                    \n",
    "                    ref_left = TableRefDTO(table_name=table_1.name, column_name=col_1.name, cardinality_type=cardinality_col_1, relationship_type=col_1_relationship)\n",
    "                    ref_right = TableRefDTO(table_name=table_2.name, column_name=col_2.name, cardinality_type=cardinality_col_2, relationship_type=col_2_relationship)\n",
    "                    dependency = DependencyDTO(ref_left, ref_right, dependency_source='Discovered')\n",
    "                    tables_dependencies.append(dependency)\n",
    "                    \n",
    "                    set_col_keytype_to_FK(table_1, col_1.name, 'Discovered')\n",
    "                    set_col_keytype_to_FK(table_2, col_2.name, 'Discovered')\n",
    "                    \n",
    "                    print(dependency.left.tableName+'.'+dependency.left.columnName+'_'+dependency.left.cardinalityType+'_'+dependency.left.relationshipType+'  '+\\\n",
    "                          dependency.right.tableName+'.'+dependency.right.columnName+'_'+dependency.right.cardinalityType+'_'+dependency.right.relationshipType+'  '+dependency.dependencySource)\n",
    "\n",
    "    return tables_dependencies\n",
    "\n",
    "                    \n",
    "# Iterate over thr final dependencies_list and remove from it duplicate dependencies\n",
    "def remove_duplicate_dependencies(dependencies_list):\n",
    "    all_dep_hash = []\n",
    "    new_dependencies_list = []\n",
    "    for dep in dependencies_list:\n",
    "        ref_hash_left = dep.left.tableName + '_' + dep.left.columnName \n",
    "        ref_hash_right = dep.right.tableName + '_' + dep.right.columnName\n",
    "        \n",
    "        left_right = ref_hash_left + '_' + ref_hash_right\n",
    "        right_left = ref_hash_right + '_' + ref_hash_left\n",
    "        \n",
    "        if (left_right not in all_dep_hash) & (right_left not in all_dep_hash):\n",
    "            new_dependencies_list.append(dep)\n",
    "            all_dep_hash = all_dep_hash + [left_right, right_left]\n",
    "        \n",
    "    return new_dependencies_list\n",
    "\n",
    "# Change all 'identifier code' type columns that didnt recognized as PK or FK to 'lable' type\n",
    "def change_identifier_code_to_lable(schema_discovery):\n",
    "    for table in schema_discovery.tables:\n",
    "        for col in table.columns:\n",
    "            if ((not col.isPK) & (not col.isFK) & (col.type == 'identifier code')):\n",
    "                col.type = 'lable'\n",
    "            \n",
    "# Iterate over known dependencies and add them to dependencies_list that evntually will be returned and added to \n",
    "# schema_discovery.dependencies\n",
    "def add_table_dependencies_from_metadata(table, table_metadata, schema_discovery, data_path):\n",
    "    table_constraints = list(table_metadata.constraints)\n",
    "    dependencies_list = []\n",
    "\n",
    "    for i in range(len(table_constraints)):\n",
    "        if type(table_constraints[i]) == sqlalchemy.sql.schema.ForeignKeyConstraint:\n",
    "            table_columns_with_foreign_key = []\n",
    "            \n",
    "            # get foreign keys names as thay appear in the current table\n",
    "            for col in table_constraints[i].columns:\n",
    "                table_columns_with_foreign_key.append((str(col.table.name), col.name))\n",
    "            \n",
    "            # get foreign keys names as thay appear in the foreign table\n",
    "            all_foreign_keys = [(elemnt.column.table.name, elemnt.column.name) for elemnt in table_constraints[i].elements]\n",
    "            \n",
    "            if len(table_columns_with_foreign_key) != len(all_foreign_keys):\n",
    "                return dependencies_list\n",
    "            \n",
    "            # iterate over all pairs of columns we found (current table columns, foreign table colum)\n",
    "            for i in range(len(table_columns_with_foreign_key)):\n",
    "                table_col = table_columns_with_foreign_key[i]\n",
    "                foreign_col = all_foreign_keys[i]\n",
    "                \n",
    "                foreign_table = get_table_by_name(schema_discovery, foreign_col[0])\n",
    "                if foreign_table == None:\n",
    "                    return dependencies_list\n",
    "                \n",
    "                table_data = get_data(data_path, table, 0, to_sample=False)\n",
    "                foreign_table_data = get_data(data_path, foreign_table, 0, to_sample=False)\n",
    "                \n",
    "                cardinality_left = get_cardinality(table_data, table_col[1])\n",
    "                cardinality_right = get_cardinality(foreign_table_data, foreign_col[1])\n",
    "                \n",
    "                col_1_relationship, col_2_relationship = get_relationship_type(table_data, foreign_table_data, table_col[1], foreign_col[1])\n",
    "                \n",
    "                ref_left = TableRefDTO(table_name=table_col[0], column_name=table_col[1], cardinality_type=cardinality_left, relationship_type=col_1_relationship)\n",
    "                ref_right = TableRefDTO(table_name=foreign_col[0], column_name=foreign_col[1], cardinality_type=cardinality_right, relationship_type=col_2_relationship)\n",
    "                dependency = DependencyDTO(ref_left, ref_right, dependency_source='Metadata')\n",
    "    \n",
    "                dependencies_list.append(dependency)\n",
    "                \n",
    "                set_col_keytype_to_FK(table, table_col[1], 'Metadata')\n",
    "                set_col_keytype_to_FK(foreign_table, foreign_col[1], 'Metadata')\n",
    "\n",
    "                print(dependency.left.tableName+'.'+dependency.left.columnName+'_'+dependency.left.cardinalityType+'_'+dependency.left.relationshipType+'  '+\\\n",
    "                      dependency.right.tableName+'.'+dependency.right.columnName+'_'+dependency.right.cardinalityType+'_'+dependency.right.relationshipType+'  '+dependency.dependencySource)\n",
    "                    \n",
    "    return dependencies_list \n",
    "\n",
    "\n",
    "# Main function; extract dependencies between each possible combonation of two table from schema_discovery\n",
    "def get_external_dependencies(schema_discovery, data_path, metadata=None):\n",
    "    all_tables_dependencies = []\n",
    "    if metadata != None:\n",
    "        for table in schema_discovery.tables:\n",
    "            table_metadata = metadata.tables[table.name]\n",
    "            table_dep_from_metadata = add_table_dependencies_from_metadata(table, table_metadata, schema_discovery, data_path)\n",
    "            all_tables_dependencies = all_tables_dependencies + table_dep_from_metadata\n",
    "    \n",
    "    all_possible_table_combinations = [x for x in itertools.combinations(schema_discovery.tables, 2)]\n",
    "\n",
    "    for tables_tupple in all_possible_table_combinations:\n",
    "        table_1 = tables_tupple[0]\n",
    "        table_2 = tables_tupple[1]\n",
    "        \n",
    "        tables_dependencies = find_foreign_keys(table_1, table_2, data_path)\n",
    "        all_tables_dependencies = all_tables_dependencies + tables_dependencies\n",
    "      \n",
    "    all_tables_dependencies = remove_duplicate_dependencies(all_tables_dependencies)\n",
    "    schema_discovery.dependencies = all_tables_dependencies\n",
    "    change_identifier_code_to_lable(schema_discovery)\n",
    "    \n",
    "get_external_dependencies(schema_discovery, data_path, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all tables names\n",
    "\n",
    "for table in schema_discovery.tables:\n",
    "    print(table.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each table print columns names, types, raw types, isPK and isFK\n",
    "\n",
    "for table in schema_discovery.tables:\n",
    "    print(table.name)\n",
    "    for col in table.columns:\n",
    "        print(col.name + ' ' +  col.type + ' ' + col.rawType + ' ' + str(col.isPK) + ' ' + str(col.PKsource) + ' ' + str(col.isFK) + ' ' + str(col.FKsource))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each table print its entities\n",
    "\n",
    "for table in schema_discovery.tables:\n",
    "    print(table.name)\n",
    "    print('NUMBER OF ENTITIES:', len(table.entities))\n",
    "    for entity in table.entities:\n",
    "        print(entity.columns)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all dependencies between tables\n",
    "\n",
    "i = 0\n",
    "for dependency in schema_discovery.dependencies:\n",
    "    print(dependency.left.tableName+'.'+dependency.left.columnName+'_'+dependency.left.cardinalityType+'_'+dependency.left.relationshipType+'  '+\\\n",
    "          dependency.right.tableName+'.'+dependency.right.columnName+'_'+dependency.right.cardinalityType+'_'+dependency.right.relationshipType+'  '+dependency.dependencySource)\n",
    "    i+=1\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_schema_discovery(schema_discovery,'./schema_discovery.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "auto_discovery",
  "notebookId": 1651889223852077
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
