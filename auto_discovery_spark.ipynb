{"cells":[{"cell_type":"code","source":["# NEED TO INSTALL THIS PACKAGES BEFORE RUN"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["#%sh\n#pip install python-louvain\n#pip install sqlalchemy"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Collecting python-louvain\n  Downloading https://files.pythonhosted.org/packages/31/d4/d244fa4ca96af747b9204ce500e8919ce72eab60de2129b22a45434f1598/python-louvain-0.14.tar.gz\nRequirement already satisfied: networkx in /databricks/conda/envs/databricks-ml/lib/python3.7/site-packages (from python-louvain) (2.2)\nRequirement already satisfied: numpy in /databricks/conda/envs/databricks-ml/lib/python3.7/site-packages (from python-louvain) (1.16.2)\nRequirement already satisfied: decorator&gt;=4.3.0 in /databricks/conda/envs/databricks-ml/lib/python3.7/site-packages (from networkx-&gt;python-louvain) (4.4.0)\nBuilding wheels for collected packages: python-louvain\n  Building wheel for python-louvain (setup.py): started\n  Building wheel for python-louvain (setup.py): finished with status &#39;done&#39;\n  Stored in directory: /root/.cache/pip/wheels/e7/8d/24/6b3a464bb23e96ecba3f68868e85721534fd8158a9cd7b426b\nSuccessfully built python-louvain\nInstalling collected packages: python-louvain\nSuccessfully installed python-louvain-0.14\nCollecting sqlalchemy\n  Downloading https://files.pythonhosted.org/packages/5c/8d/81d91c13e7f358e8c2b84730d5c09aec1dbbcf190e5b64c41cc5c67dfa4a/SQLAlchemy-1.3.17-cp37-cp37m-manylinux2010_x86_64.whl (1.2MB)\nmlflow 1.5.0 requires alembic, which is not installed.\nmlflow 1.5.0 requires prometheus-flask-exporter, which is not installed.\nInstalling collected packages: sqlalchemy\nSuccessfully installed sqlalchemy-1.3.17\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# IMPORTS\n\nfrom pyspark.sql import SQLContext\nfrom  pyspark.sql.functions import input_file_name, concat, col, lit, countDistinct, when, count, isnan, length\nfrom pyspark.sql.types import StringType\nfrom sqlalchemy import create_engine, MetaData\nimport sqlalchemy\nimport pandas as pd\nimport numpy as np\nimport threading\nimport json\nimport inspect\nimport itertools\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nimport networkx as nx\nimport community\nimport seaborn as sns\nfrom dateutil.parser import parse"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/databricks/python/lib/python3.7/site-packages/networkx/classes/graph.py:23: DeprecationWarning: Using or importing the ABCs from &#39;collections&#39; instead of from &#39;collections.abc&#39; is deprecated, and in 3.8 it will stop working\n  from collections import Mapping\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Serealize result to json\nclass ObjectEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if hasattr(obj, \"to_json\"):\n            return self.default(obj.to_json())\n        elif hasattr(obj, \"__dict__\"):\n            d = dict(\n                (key, value)\n                for key, value in inspect.getmembers(obj)\n                if not key.startswith(\"__\")\n                and not inspect.isabstract(value)\n                and not inspect.isbuiltin(value)\n                and not inspect.isfunction(value)\n                and not inspect.isgenerator(value)\n                and not inspect.isgeneratorfunction(value)\n                and not inspect.ismethod(value)\n                and not inspect.ismethoddescriptor(value)\n                and not inspect.isroutine(value)\n            )\n            return self.default(d)\n        return obj\n      \n# Stores the connection parameters\nclass Connection:\n    def __init__(self,sqlc, jdbcUsername, jdbcPassword, jdbcDriver, jdbcHostname, jdbcDatabase, port):\n      self.sqlc = sqlc\n      self.jdbcUsername = jdbcUsername\n      self.jdbcPassword = jdbcPassword\n      self.jdbcDriver = jdbcDriver\n      self.jdbcHostname = jdbcHostname\n      self.jdbcDatabase = jdbcDatabase\n      self.port = port\n      self.jdbcUrl = \"jdbc:postgresql://{0}/{1}\".format(jdbcHostname, jdbcDatabase)\n      \n# SchemaDiscoveryDTO stores a list of tables in the input connection (tables of type Table) and list of dependencies between \n# the tables\nclass SchemaDiscoveryDTO:\n    def __init__(self, name):\n        self.name = name\n        self.tables = []\n        self.dependencies = None\n        \n# TableDTO contains a name, schema name (e.g. public), list of columns and list of entities (each entity is list of columns)\nclass TableDTO:\n    def __init__(self, table_name):       \n        self.name = table_name\n        self.columns = []  \n        self.entities = None\n        self.size = None\n        \n# A column contains column name, raw_type(int, float, object) and type(label/free text/ numeric/ timestamp/ identifier code)\nclass ColumnDTO:\n    def __init__(self, col_name, col_raw_type, col_type, unique_cnt, not_null_cnt, min_digits, max_digits, is_pk, pk_source, is_fk, fk_source):\n        self.name = col_name\n        self.rawType = col_raw_type\n        self.type = col_type\n        self.uniqueCount = unique_cnt\n        self.notNullCount = not_null_cnt\n        self.minDigits = min_digits\n        self.maxDigits = max_digits\n        self.isPK = is_pk\n        self.PKsource = pk_source\n        self.isFK = is_fk\n        self.FKsource = fk_source\n        \n        \n# A Entity contains a list of columns \nclass EntityDTO:\n    def __init__(self):\n        self.columns = []\n        \n# TableRefDTO contain name of the table, name of the column, its cardinality and its relationship\nclass TableRefDTO:\n    def __init__(self, table_name, column_name, key_type, cardinality_type, relationship_type):\n        self.tableName = table_name\n        self.columnName = column_name\n        self.keyType = key_type\n        self.cardinalityType = cardinality_type\n        self.relationshipType = relationship_type\n\n# DependencyDTO contain two TableRefDTO; left and right that represent the dependency between two columns and the source of the dependency (from metadata or founde by us)\nclass DependencyDTO:\n    def __init__(self, table_left, table_right, dependency_source):\n        self.left = table_left\n        self.right = table_right\n        self.dependencySource = dependency_source"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# GLOBAL FUNCTIONS\n\n# Read data with given table.table_name as parquet file and sample sample_size records if to_sample == True\ndef get_data(table, sample_size, to_sample=True):  \n    df_pqt = spark.table(table.name)\n    \n    if to_sample:\n        table_size = table.size\n        how_many_take = min(table_size, sample_size)\n        sampeld_records = df_pqt.sample(fraction=0.1 + 1.0*how_many_take/(table_size*0.9), withReplacement=False).limit(how_many_take)\n\n        return sampeld_records\n    \n    return df_pqt\n  \n# Save schema_discovery object in given location\ndef save_schema_discovery(schema_discovery, schema_discovery_file_location):\n    with open('/dbfs/tmp/schema_discovery_with_entities_and_dependencies.json', 'w', encoding='utf-8') as f:\n        json.dump(schema_discovery, cls=ObjectEncoder, indent=2, fp=f, ensure_ascii=False)\n    \n    dbutils.fs.cp(\"dbfs:/tmp/schema_discovery_with_entities_and_dependencies.json\", schema_discovery_file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# STEP 0 - ETL - READ DATA FROM GIVEN DATABASE OR CSV FILES AND SAVE LOCALY AS PARQUET FILES\n\nclass ETL:\n    schema_discovery = None\n    metadata = None\n    \n    # Create connection with given connection details\n    def get_connection(self, jdbcUsername, jdbcPassword, jdbcDriver, jdbcHostname, jdbcDatabase, port):\n        sqlc = SQLContext(sc)\n        spark.conf.set(\"fs.azure.account.key.homecredittest01.blob.core.windows.net\", dbutils.secrets.get(scope = \"blobs\", key = \"hcaccesskey\"))\n        connection = Connection(sqlc, jdbcUsername, jdbcPassword, jdbcDriver, jdbcHostname, jdbcDatabase, port)    \n        return connection\n\n    def get_metadata(self, connection):\n        # construct an engine connection string\n        engine_string = \"postgresql+psycopg2://{user}:{password}@{host}:{port}/{database}\".format(user = connection.jdbcUsername,\n                                                                                                  password = connection.jdbcPassword,\n                                                                                                  host = connection.jdbcHostname,\n                                                                                                  port = connection.port,\n                                                                                                  database = connection.jdbcDatabase,)\n\n        # create sqlalchemy engine\n        engine = create_engine(engine_string)\n        metadata = None\n        try:\n            metadata = MetaData(bind=engine, reflect=True)\n        except:\n            print('cant read metadata from DB')\n\n        self.metadata = metadata\n\n    # Read data with table.name from given connection and save it as parquet file\n    # sample_percentage can be number between 0-1 (the percentage of data to sample from the original data)\n    def convert_table_from_DB_to_pqt(self, connection, table, sample_percentage):\n        if sample_percentage == 1:\n            sql_str = '(SELECT * FROM ' + self.schema_discovery.name+'.'+table.name + ') a'\n        else:\n            sql_str = '(SELECT * FROM ' + self.schema_discovery.name+'.'+table.name + ' where random() <= ' + str(sample_percentage) + ') a'\n\n        data_db = connection.sqlc.read.format(\"jdbc\")\\\n                            .option(\"driver\", connection.jdbcDriver)\\\n                            .option(\"url\", connection.jdbcUrl)\\\n                            .option(\"dbtable\", sql_str)\\\n                            .option(\"user\", connection.jdbcUsername)\\\n                            .option(\"password\", connection.jdbcPassword)\\\n                            .option(\"sslmode\",\"require\")\\\n                            .option(\"numPartitions\",4)\\\n                            .option(\"fetchsize\",1000)\\\n                            .load()\n\n        data_db.write.mode(\"overwrite\").saveAsTable(table.name) #.option(\"path\",schema_discovery.name)\n        spark.sql(\"REFRESH TABLE \" + table.name)\n        data_df = spark.table(table.name)\n        table.size  = data_df.count()\n\n\n    # Get all tables names from table with all tables details and for each table:\n    # 1. read the table and save it as parquet file using convert_table_from_DB_to_pqt function\n    # 2. save their names and size in schema_discovery.tables_list\n    def create_schema_discovery_from_DB(self, jdbcUsername, jdbcPassword, jdbcDriver, jdbcHostname, jdbcDatabase, port, sample_percentage):\n        # Create connection to DB  \n        connection = self.get_connection(jdbcUsername, jdbcPassword, jdbcDriver, jdbcHostname, jdbcDatabase, port)\n        \n      \n        df_tables = connection.sqlc.read.format(\"jdbc\")\\\n                              .option(\"driver\", connection.jdbcDriver)\\\n                              .option(\"url\", connection.jdbcUrl)\\\n                              .option(\"dbtable\", \"pg_stat_user_tables\")\\\n                              .option(\"user\", connection.jdbcUsername)\\\n                              .option(\"password\", connection.jdbcPassword)\\\n                              .option(\"sslmode\",\"require\")\\\n                              .option(\"numPartitions\",4)\\\n                              .option(\"fetchsize\",1000)\\\n                              .load()\n\n        df_tables = df_tables[['schemaname','relname']].toPandas()\n        schema_neme = df_tables['schemaname'].values[0]\n        self.schema_discovery = SchemaDiscoveryDTO(schema_neme)\n\n        threads_list = []\n        for table_details in df_tables.itertuples():\n            table = TableDTO(table_details.relname)\n            self.schema_discovery.tables.append(table)\n            thread = threading.Thread(target=self.convert_table_from_DB_to_pqt, args=(connection, table, sample_percentage,))\n            threads_list.append(thread)\n            thread.start()\n\n        for thread in threads_list:\n            thread.join()\n            \n        self.get_metadata(connection)\n        \n        return self.schema_discovery, self.metadata\n\n    # Read data with table.table_name from given path and save it as parquet file\n    # sample_percentage can be number between 0-1 (the percentage of data to sample from the original data)\n    def convert_table_from_csv_to_pqt(self, path, table, sample_percentage):\n        data_file = spark.read.csv(path + table.name + '.csv', header='true', inferSchema='true')\n        if sample_percentage != 1:\n            data_file = data_file.sample(withReplacement=False, fraction=sample_percentage)\n\n        #Remove illegal characters in column names\n        new_column_name_list= list(map(lambda x: x.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"_\")\n                                                  .replace(\".\", \"_\").replace(\",\", \"_\").replace(\"{\", \"_\").replace(\"}\", \"_\"), data_file.columns))\n        data_file = data_file.toDF(*new_column_name_list)\n        data_file.write.mode(\"overwrite\").option(\"path\", self.schema_discovery.name).saveAsTable(table.name) \n        spark.sql(\"REFRESH TABLE \" + table.name)\n        table.size  = spark.table(table.name).count()\n\n    # Get all file names from the given folder (e.g. path = 'wasbs://hc-test-data-01@homecredittest01.blob.core.windows.net/hc-test-01/'), \n    # for each file:\n    # 1. read the file (table) and save it as parquet file using convert_table_from_DB_to_pqt function\n    # 2. save their names and size in schema_discovery.tables_list\n    def create_schema_discovery_from_csv(self, path, sample_percentage):\n        df_files = spark.read.format(\"csv\").load(path).select(input_file_name()).distinct().toPandas()['input_file_name()']\n        #Filter csv files only\n        df_files = [ filename for filename in df_files if filename.endswith( 'csv' ) ]\n\n        #Extract file name and (last) directory name as table and schema correspondingly\n        res = [ (path.split('/')[-2] , path.split('/')[-1].split('.')[-2]) if path.index('.')> 0 else (path.split('/')[-2],path.split('/')[-1]) for path in df_files ]\n        schemas,fnames = zip(*res)\n        df_tables = pd.DataFrame(data={'schemaname': schemas, 'relname': fnames})\n\n        schema_name = df_tables['schemaname'].values[0]\n        self.schema_discovery = SchemaDiscoveryDTO(schema_name)\n\n        threads_list = []\n        for table_details in df_tables.itertuples():\n            table = Table(table_details.relname)\n            self.schema_discovery.tables.append(table)\n            thread = threading.Thread(target=self.convert_table_from_csv_to_pqt, args=(path, table, sample_percentage,))\n            threads_list.append(thread)\n            thread.start()\n\n        for thread in threads_list:\n            thread.join()\n            \n        return self.schema_discovery, None"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# STEP 1 - EXTRACT COLUMNS TYPES\n\nclass ColumnsInfoExtractor:\n\n    def __init__(self, schema_discovery, metadata):\n        self.schema_discovery = schema_discovery\n        self.metadata = metadata\n    \n    # Check if given string is in date format\n    def is_date(self, string):\n        try:\n            parse(string, fuzzy=False)\n            return True\n        except:\n            return False\n\n    # An auxiliary function that accepts basic columns_data (name & raw data type per column) and a sample of records (based on these columns) \n    # and calculates the final column type (label/free text/ numeric/ timestamp/ identifier code)\n    def calc_col_types(self, columns_data, data, table_metadata):   \n        # Calculates data types from sampled_records and updating columns_data accordingly\n        data_size = data.count() # the maximal number of unique values\n        long_str = 200 # A column containing strings with more than this number of characters will be considered free text column\n        label_threshold = 0.2 # labels are expected to contain unique values up to this percent of the number of non empty values\n        text_threshold = 0.8 # Free texts are expected to contain unique values of at least this percent of the number of non empty values\n        idenifier_threshold = 4 # Free texts are expected to contain at least this number of digits\n\n        res = columns_data.copy()\n        res['raw_type'] = res['raw_type'].astype(str)\n\n        # Add number of unique values per column\n        res['unique_vals'] = [val for val in data.agg(*(countDistinct(c).alias(c) for c in data.columns)).collect()[0]]\n\n        # Add number of non-null values per column\n        res['not_null_cnt'] = [val for val in data.select([count(c).alias(c) for c in data.columns]).collect()[0]]\n\n        res['min_digits'] = 0 \n        res['max_digits'] = 0 \n        res['is_bool'] = [False] * len(res)\n        res['is_str'] = [False] * len(res)\n        res['is_date'] = [False] * len(res)\n        for col in data.schema.names:\n            col_length = data.withColumn('len', length(col)).select('len')\n            max_str_len = col_length.agg({\"len\": \"max\"}).collect()[0][\"max(len)\"]\n            min_str_len = col_length.agg({\"len\": \"min\"}).collect()[0][\"min(len)\"]\n            res.loc[res['col_name']==col, 'max_digits'] = max_str_len\n            res.loc[res['col_name']==col, 'min_digits'] = min_str_len\n\n            col_vals_as_str = [val[0] for val in data.select(col).dropna().withColumn(col, data[col].cast(StringType())).sample(False, 0.1).limit(1000000).collect()]\n            # check if all values are None\n            if len(col_vals_as_str) == 0:\n                continue\n\n            if ('True' in col_vals_as_str) | ('False' in col_vals_as_str):\n                res.loc[res['col_name']==col, 'is_bool'] = True\n\n            if not any(any(sub_str.isdigit() for sub_str in main_str) for main_str in col_vals_as_str):\n                res.loc[res['col_name']==col, 'is_str'] = True\n\n            date_check = [self.is_date(val) for val in col_vals_as_str]    \n            if all(date_check):\n                res.loc[res['col_name']==col, 'is_date'] = True    \n\n        res['col_type'] = res['raw_type'].copy()\n\n        # Checks if the column is numeric\n        is_numeric = np.vectorize(lambda x: True if any(sub_str in str(x) for sub_str in ['int', 'float']) else False)\n        res['is_numeric'] = is_numeric(res['raw_type']) \n\n        # Checks if the column is int\n        is_int = np.vectorize(lambda x: True if 'int' in str(x) else False)\n        res['is_int'] = is_int(res['raw_type'])\n\n        # Recognizing identifing keys as unique numbers or codes\n        msk_id_code = ((res['is_str']==False) & (res['is_bool']==False) & (res['min_digits'] == res['max_digits']) & (res['min_digits']>=idenifier_threshold) &\\\n                       ((res['is_int']==True) | (res['raw_type'] == 'object')))\n        res.loc[msk_id_code,'col_type'] = 'identifier code' \n\n        # Recognizing labels as repeating short texts\n        msk_lable = ((res['is_bool']==False) & (res['col_type'] == res['raw_type']) & \\\n                     (res['unique_vals']<res['not_null_cnt']*label_threshold) & (res['raw_type'] == 'object'))\n        res.loc[msk_lable,'col_type'] = 'label' \n\n        # Recognizing numeric columns as numeric columns that are not identifiers\n        msk_numeric = (res['is_bool']==False) & (res['col_type'] == res['raw_type']) & (res['is_numeric'])\n        res.loc[msk_numeric,'col_type'] = 'numeric'\n\n        # Recognizing free texts as unique or long texts\n        msk_free_text = ((res['is_bool']==False) & (res['col_type'] == res['raw_type']) & \\\n                         ((res['is_str']==True) | (((res['unique_vals']>=res['not_null_cnt']*text_threshold) | (res['max_digits']>=long_str)) \\\n                                                   & (res['raw_type'] == 'object'))))\n        res.loc[msk_free_text,'col_type'] = 'free text' \n\n        # Recognizing bool columns as bool columns\n        msk_bool = (res['is_bool'])\n        res.loc[msk_bool,'col_type'] = 'bool'\n\n        # Recognizing dates\n        msk_date = ((res['raw_type'].str.contains('time')) | (res['is_date'] == True))\n        res.loc[msk_date,'col_type'] = 'timestamp'\n\n        # Recognizing Primery Key\n        res['is_pk'] = False\n        res['pk_source'] = 'NONE'\n        if table_metadata != None:\n            for col in [col for col in table_metadata.columns if ((col.name in columns_data['col_name'].values) & (col.primary_key))]:\n                res.loc[res['col_name'] == col.name, 'is_pk'] = True\n                res.loc[res['col_name'] == col.name, 'pk_source'] = 'Metadata'\n\n        else:\n            msk_pk = ((res['col_type']=='identifier code') & (res['unique_vals'] == res['not_null_cnt']) & (res['unique_vals'] == data_size))\n            res.loc[msk_pk, 'is_pk'] = True \n            res.loc[msk_pk, 'pk_source'] = 'Discovered' \n\n            num_pk = res.loc[res['is_pk'] == True].shape[0]\n\n            # If no PK, search for combinations of identifier code columns that create x unqiue keys where x is the size of the data\n            if num_pk == 0:\n                id_code_cols = res.loc[(res['col_type'] == 'identifier code') & (res['not_null_cnt'] == data_size)]['col_name'].values\n                for pk_len in range(2, len(id_code_cols)+1):\n                    all_possible_combination = itertools.combinations(id_code_cols, pk_len)\n                    for pk_group in all_possible_combination:\n                        # check if for each possible pk_group with unique values we have only one record --> PK\n                        num_unqiue_vals = data.groupBy(list(pk_group)).count().count()\n                        if num_unqiue_vals == data_size:\n                            res.loc[res['col_name'].isin(pk_group), 'is_pk'] = True\n                            res.loc[res['col_name'].isin(pk_group), 'pk_source'] = 'Discovered'\n                            pk_len = len(id_code_cols)+1\n                            break \n\n            # If more then one column acts as PK (by itself, not as combination of columns) we need to choose only one of \n            # them as PK and the others will be regular identifier code columns (there is no additional information in keeping all as PK)\n            elif num_pk > 1:\n                all_pk_cols = res.loc[res['is_pk'] == True]['col_name']\n                res.loc[res['col_name'].isin(all_pk_cols[1:]), 'is_pk'] = False\n                res.loc[res['col_name'].isin(all_pk_cols[1:]), 'pk_source'] = 'NONE'\n\n        return res\n\n\n    # Extracting the list of columns with column names and their raw data types from table with given table.name\n    def get_columns(self, table, table_metadata): \n        data = get_data(table, 0, to_sample=False)\n\n        cols_dtype = data.dtypes\n        col_names = [val[0] for val in cols_dtype]\n        col_types = [val[1] for val in cols_dtype]\n        cols_df = pd.DataFrame({'col_name':col_names, 'raw_type':col_types})\n\n        cols_data = self.calc_col_types(cols_df, data, table_metadata)\n\n        table.columns = []\n        for idx, col in cols_data.iterrows():\n            # filter free text columns        \n            if str(col['col_type']) != 'free text':\n                table.columns.append(ColumnDTO(col['col_name'], str(col['raw_type']), str(col['col_type']), col['unique_vals'], col['not_null_cnt'],\n                                               col['min_digits'], col['max_digits'], col['is_pk'], col['pk_source'], False, 'NONE'))\n\n    # Extracting columns details for each table at schema_discovery by calculations made on a given data of each table              \n    def extract_columns_info(self):    \n        threads_list = []\n        for table in self.schema_discovery.tables:\n            table_metadata = None\n            if self.metadata != None:\n                table_metadata = self.metadata.tables[table.name]\n\n            thread = threading.Thread(target=self.get_columns, args=(table, table_metadata,))\n            threads_list.append(thread)\n            thread.start()\n\n        for thread in threads_list:\n            thread.join()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# STEP 2 - EXTRACT ENTTIES\n\nclass EntitiesExtractor:\n  \n    def __init__(self, schema_discovery):\n        self.schema_discovery = schema_discovery\n\n    # Transform all columns values to categorical values (e.g. from ['israel', 'usa', 'israel', 'spain'] to [1, 2, 1, 3])\n    def to_categorical(self, data):\n        le = preprocessing.LabelEncoder()\n        categorical_cols = {}\n        for col_name in data.schema.names:\n            col_real_vals = [x[col_name] for x in data.select(col_name).collect()]\n            col_real_vals = ['None' if val is None else val for val in col_real_vals]\n            categorical_cols[col_name] = le.fit_transform(col_real_vals)\n        return categorical_cols\n\n    # Calculate correlation matrix; extracts internal dependencies between column pairs that appear in data\n    # Matrix corr_matrix contain the dependencies estimation between col_1 and col_2 (where col_1 != col_2) using mutual information measure\n    def get_correlation_matrix(self, table, data, results_dict):    \n        categorical_cols = self.to_categorical(data)\n        data_col_names = data.schema.names\n        all_possible_cols_combinations = [x for x in itertools.combinations(data_col_names, 2)]\n\n        corr_matrix = pd.DataFrame(columns=data_col_names, \n                                   index=data_col_names, \n                                   data=np.zeros((len(data_col_names), len(data_col_names))))\n\n        for col_tuple in all_possible_cols_combinations:\n            col_1 = col_tuple[0]\n            col_2 = col_tuple[1]\n\n            cat_col_1 = categorical_cols[col_1]\n            cat_col_2 = categorical_cols[col_2]\n\n            # Calculate the Information Gain of target columns given the source column\n            mutulal_info = metrics.normalized_mutual_info_score(cat_col_1, cat_col_2)\n            corr_matrix.loc[col_1, col_2] = mutulal_info\n            corr_matrix.loc[col_2, col_1] = mutulal_info\n\n        # # plot heatmap of correlation matrix\n        # sns.heatmap(corr_matrix, xticklabels=corr_matrix.columns, yticklabels=corr_matrix.columns, annot=True)\n\n        results_dict[table.name] = corr_matrix\n\n    # Create network graph using networkx package based on correlation matrix created by get_correlation_matrix function on given data\n    # when filter_col is True -> correlation value is set to 0 for columns tuple with correlation value lower then mean correlation value \n    def get_network_graph(self, corr_matrix, filter_col=True):\n        links = corr_matrix.stack().reset_index()\n        links.columns = ['var1', 'var2', 'value']\n\n        # Remove self correlation\n        links_filtered = links.loc[links['var1'] != links['var2']]\n\n        if filter_col:\n            # Keep only correlation over a threshold (the mean correlation value)\n            mean_corr = links_filtered.loc[links_filtered['value'] > 0]['value'].mean()\n            links_filtered = links_filtered.loc[links_filtered['value'] > mean_corr]\n\n        # Build the graph\n        G = nx.from_pandas_edgelist(links_filtered, 'var1', 'var2')\n\n        # # Plot the network:\n        # nx.draw_circular(G, with_labels=True, node_size=200, font_size=10)\n\n        return G\n\n    # Detects communities in the graph\n    # The communities detection method we use can be one of the next communities detection methods:\n    # best_partition (based on Louvain algorithm)\n    def detect_communities(self, G):\n        communities = community.best_partition(G)\n\n        clusters = []\n        for i in range(len(G.nodes)):\n            curr_cluster = [key for key, val in communities.items() if val == i]\n            if len(curr_cluster) == 0:\n                break\n            clusters.insert(i, curr_cluster)\n\n        return clusters\n\n    # Plot the entities we found as communities on network (features) graph\n    def plot_communty_network(self, G, path_to_save_plot, with_labels=True):\n        plt.figure(figsize=(8,8))\n        pos = nx.spring_layout(G, k=2)\n        node_colors = ['green', 'red', 'yellow', 'black', 'blue', 'orange', 'pink', 'purple', 'gray', 'brown']\n\n    #     edge_labels = nx.get_edge_attributes(G, 'value')\n    #     for key_tuple in edge_labels:\n    #       edge_labels[key_tuple] = round(edge_labels[key_tuple], 3)\n\n    #     nx.draw_networkx_edges(G, pos, width=1, alpha=0.9)\n    #     nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels)\n\n        edges = G.edges()\n        weights = [G[u][v]['value']*2 for u,v in edges]\n        nx.draw_networkx_edges(G, pos, width=weights, alpha=0.9)\n\n        for node, node_att_dict in G.nodes(data=True):\n            cluster_idx = node_att_dict['cluster']\n            color = node_colors[cluster_idx%len(node_colors)]\n            nx.draw_networkx_nodes(G, pos, nodelist=[node], node_color=color, node_size=150)\n            if with_labels:\n                nx.draw_networkx_labels(G, pos, {node:node}, font_size=8)\n\n        nx.write_gexf(G, path_to_save_plot + '.gexf')\n        plt.savefig(path_to_save_plot)  \n        plt.show()\n\n    # Main function that uses all previous functions to detect main entities for each table in schema_discovery\n    def find_entities(self, sample_size):\n        threads_list = []\n        results_dict = {}\n        for table in self.schema_discovery.tables:\n            data = get_data(table, sample_size)\n            thread = threading.Thread(target=self.get_correlation_matrix, args=(table, data, results_dict,))\n            threads_list.append(thread)\n            thread.start()\n\n        for thread in threads_list:\n            thread.join()\n\n        for table in self.schema_discovery.tables:\n            corr_matrix = results_dict[table.name]\n            G = self.get_network_graph(corr_matrix)\n            clusters = self.detect_communities(G)\n    #         self.plot_communty_network(G, '/dbfs/tmp/' + table.name)\n            print('finish finding clusters for table ' + table.name + ', num of clusters: ' + str(len(clusters)))\n\n            table.entities = []\n            for cluster in clusters:\n                entity = EntityDTO()\n                entity.columns = cluster\n                table.entities.append(entity)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# STRP 3 - EXTRACT EXTERNAL DEPENDENCIES\n\nclass DependenciesExtractor:\n  \n    def __init__(self, schema_discovery, metadata):\n        self.schema_discovery = schema_discovery\n        self.metadata = metadata\n\n    # Concatinate number of columns to one unique PK column - done for compersion between PK of tables that build from number\n    # of columns\n    def combain_pk_columns(self, data, pk_list):\n        data = data.withColumn('concat_pk_col', concat(col(pk_list[0]), lit('_'), col(pk_list[1])))\n        for i in range(2, len(pk_list)):\n            data = data.withColumn('concat_pk_col', concat(col(concat_pk_col), lit('_'), col(pk_list[i])))\n\n        pk_raw_types = [col.rawType for col in pk_list]\n        pk_raw_types.sort()  \n        new_pk_list = [ColumnDTO(name='concat_pk_col', \n                                 rawType='_'.join(pk_raw_types), \n                                 type='identifier code',\n                                 isPK=True,\n                                 PKsource='Discovered',\n                                 isFK=False,\n                                 FKsource='NONE')]\n\n        return data, new_pk_list\n\n\n    # Get the cardinality of column given the data. cardinality can get one of the next values:\n    # '1': each unique value of this column appear only once\n    # 'M': there is at least one unqiue value of this column that appear more then one time\n    def get_cardinality(self, data, col_name):\n        map_cardinalty = lambda x : 'One' if (x == 1) else 'Many' \n\n        counter_col = data.dropna(subset=[col_name]).groupby(col_name).count()\n        cardinality = map_cardinalty(counter_col.agg({\"count\": \"max\"}).collect()[0][\"max(count)\"])\n\n        return cardinality\n\n\n    # Search table object with given table_name in schema_discovery.tables_list. If there is no table with this name- return None\n    def get_table_by_name(self, table_name):\n        for table in self.schema_discovery.tables:\n            if table.name == table_name:\n                return table\n        return None\n\n\n    # Change the type of columns with given col_name to 'FK' type\n    def set_col_keytype_to_FK(self, table, col_name, FK_source):\n        for col in table.columns:\n            if col.name == col_name:\n                col.isFK = True\n                col.FKsource = FK_source\n                if col.isPK:\n                    return 'PK'\n                else:\n                    return 'FK'\n\n        return 'NONE'\n\n    def get_unique_values(self, data, col_name):\n        col_type = data.schema[col_name].dataType\n        col_unqiue_vals = data.select(col_name).dropna().distinct()\n        df_col_vals = spark.createDataFrame(col_unqiue_vals, col_type)        \n\n\n    # Get the relationship between two columns; Contained, Contains, Partial Overlap, Equal (same values), or None if there is\n    # no relationship between this two columns\n    def get_relationship_type(self, data_1, data_2, col_1, col_2):\n        col_name_1 = col_1 + '_1'\n        col_name_2 = col_2 + '_2'                \n\n        vals_col_1 = data_1.select(col_1).dropna().distinct().withColumnRenamed(col_1, col_name_1)\n        vals_col_2 = data_2.select(col_2).dropna().distinct().withColumnRenamed(col_2, col_name_2)\n\n        col_1_relationship = None\n        col_2_relationship = None\n\n        join_on_cols = vals_col_1.join(vals_col_2, vals_col_1[col_name_1] == vals_col_2[col_name_2], how='inner')\n\n        join_size = join_on_cols.count()\n        col_1_unq_size = vals_col_1.count()\n        col_2_unq_size = vals_col_2.count()\n\n        if (join_size == col_1_unq_size) & (join_size == col_2_unq_size):\n            col_1_relationship = 'Equal'\n            col_2_relationship = 'Equal'\n        else:\n            if join_size == col_1_unq_size:\n                col_1_relationship = 'Contained'\n                col_2_relationship = 'Contains'\n            elif join_size == col_2_unq_size:\n                col_1_relationship = 'Contains'\n                col_2_relationship = 'Contained'\n            elif join_size > 0:\n                col_1_relationship = 'Overlap'\n                col_2_relationship = 'Overlap'\n\n        return col_1_relationship, col_2_relationship\n      \n    # Get all PK columns and identifier code columns of given table\n    def get_pk_id_cols(self, table):\n        pk_table = [col for col in table.columns if col.isPK]\n        identifier_code_cols = [col for col in table.columns if ((col.type == 'identifier code') & (not col.isPK) & (col.FKsource != 'Internal'))]\n        \n        return pk_table, identifier_code_cols\n\n    # Extracts external dependencies between tabels pairs\n    # dataframe connected_tabels contains names of the two connected attributes, their tabels names, and the type of \n    # dependency (1:1, 1:M, M:1, M:N)\n    def find_foreign_keys(self, table_1, table_2, tables_dependencies_dict, idx):      \n        data_1 = get_data(table_1, 0, to_sample=False)\n        data_2 = get_data(table_2, 0, to_sample=False)\n\n        pk_table_1, identifier_code_cols_1 = self.get_pk_id_cols(table_1)\n        pk_table_2, identifier_code_cols_2 = self.get_pk_id_cols(table_2)\n\n        if (len(pk_table_1) == 0) & (len(pk_table_2) == 0):\n            return\n\n        # If the tables have diffrent size of PK we cant comper bwtween them -> check dependencies only between PK of size 1 and \n        # identifier code columns\n        if (len(pk_table_1) != len(pk_table_2)):\n            if (len(pk_table_1) > 1) & (len(pk_table_2) > 1):\n                return \n            elif len(pk_table_1) > 1:\n                pk_table_1 = []\n            elif len(pk_table_2) > 1:\n                pk_table_2 = []\n\n        # If the tables have same size of PK then we can comper them- first we concatinate all pk columns to one unique column\n        if (len(pk_table_1) == len(pk_table_2)) & (len(pk_table_1) > 1):\n            data_1, pk_table_1 = self.combain_pk_columns(data_1, pk_table_1)\n            data_2, pk_table_2 = self.combain_pk_columns(data_2, pk_table_2)    \n\n        pk_identifier_code_cols_1 = pk_table_1 + identifier_code_cols_1\n        pk_identifier_code_cols_2 = pk_table_2 + identifier_code_cols_2\n\n        for col_1 in pk_identifier_code_cols_1:\n            for col_2 in pk_identifier_code_cols_2:\n                if (not col_1.isPK) & (not col_2.isPK):\n                    continue\n\n                if col_1.rawType == col_2.rawType:\n                    col_1_relationship, col_2_relationship = self.get_relationship_type(data_1, data_2, col_1.name, col_2.name)\n\n                    if col_1_relationship != None:\n                        key_type_1 = 'PK'\n                        key_type_2 = 'PK'\n                        if (col_1_relationship == 'Contained') | (col_1_relationship == 'Overlap') | (col_1_relationship == 'Equal'):\n                            key_type_1 = self.set_col_keytype_to_FK(table_1, col_1.name, 'Discovered')\n                        if (col_2_relationship == 'Contained') | (col_2_relationship == 'Overlap') | (col_2_relationship == 'Equal'):\n                            key_type_2 = self.set_col_keytype_to_FK(table_2, col_2.name, 'Discovered')\n\n                        cardinality_col_1 = self.get_cardinality(data_1, col_1.name)\n                        cardinality_col_2 = self.get_cardinality(data_2, col_2.name)\n\n                        ref_left = TableRefDTO(table_name=table_1.name, column_name=col_1.name, key_type=key_type_1, \\\n                                               cardinality_type=cardinality_col_1, relationship_type=col_1_relationship)\n                        ref_right = TableRefDTO(table_name=table_2.name, column_name=col_2.name, key_type=key_type_2, \\\n                                                cardinality_type=cardinality_col_2, relationship_type=col_2_relationship)\n                        dependency = DependencyDTO(ref_left, ref_right, dependency_source='Discovered')\n                        tables_dependencies_dict[idx].append(dependency)\n\n    #                     print(dependency.left.tableName+'.'+dependency.left.columnName+'_'+dependency.left.keyType+'_'+dependency.left.cardinalityType+'_'+\\\n    #                           dependency.left.relationshipType+'  '+\\\n    #                           dependency.right.tableName+'.'+dependency.right.columnName+'_'+dependency.right.keyType+'_'+\\\n    #                           dependency.right.cardinalityType+'_'+dependency.right.relationshipType+'  '+\\\n    #                           dependency.dependencySource)\n\n\n    # Iterate over thr final dependencies_list and remove from it duplicate dependencies\n    def remove_duplicate_dependencies(self, dependencies_list):\n        all_dep_hash = []\n        new_dependencies_list = []\n        for dep in dependencies_list:\n            ref_hash_left = dep.left.tableName + '_' + dep.left.columnName \n            ref_hash_right = dep.right.tableName + '_' + dep.right.columnName\n\n            left_right = ref_hash_left + '_' + ref_hash_right\n            right_left = ref_hash_right + '_' + ref_hash_left\n\n            if (left_right not in all_dep_hash) & (right_left not in all_dep_hash):\n                new_dependencies_list.append(dep)\n                all_dep_hash = all_dep_hash + [left_right, right_left]\n\n        return new_dependencies_list\n\n\n    # Change all 'identifier code' type columns that didnt recognized as PK or FK to 'lable' type\n    def change_identifier_code_to_lable(self):\n        for table in self.schema_discovery.tables:\n            for col in table.columns:\n                if ((not col.isPK) & (not col.isFK) & (col.type == 'identifier code')):\n                    col.type = 'lable'\n\n\n    # Iterate over known dependencies and add them to dependencies_list that evntually will be returned and added to \n    # schema_discovery.dependencies\n    def add_table_dependencies_from_metadata(self, table):\n        table_metadata = self.metadata.tables[table.name]\n        table_constraints = list(table_metadata.constraints)\n        dependencies_list = []\n\n        for i in range(len(table_constraints)):\n            if type(table_constraints[i]) == sqlalchemy.sql.schema.ForeignKeyConstraint:\n                table_columns_with_foreign_key = []\n\n                # get foreign keys names as thay appear in the current table\n                for col in table_constraints[i].columns:\n                    table_columns_with_foreign_key.append((str(col.table.name), col.name))\n\n                # get foreign keys names as thay appear in the foreign table\n                all_foreign_keys = [(elemnt.column.table.name, elemnt.column.name) for elemnt in table_constraints[i].elements]\n\n                if len(table_columns_with_foreign_key) != len(all_foreign_keys):\n                    return dependencies_list\n\n                # iterate over all pairs of columns we found (current table columns, foreign table colum)\n                for i in range(len(table_columns_with_foreign_key)):\n                    table_col = table_columns_with_foreign_key[i]\n                    foreign_col = all_foreign_keys[i]\n\n                    foreign_table = self.get_table_by_name(foreign_col[0])\n                    if foreign_table == None:\n                        return dependencies_list\n\n                    table_data = get_data(table, 0, to_sample=False)\n                    foreign_table_data = get_data(foreign_table, 0, to_sample=False)\n\n                    cardinality_left = self.get_cardinality(table_data, table_col[1])\n                    cardinality_right = self.get_cardinality(foreign_table_data, foreign_col[1])\n\n                    col_1_relationship, col_2_relationship = self.get_relationship_type(table_data, foreign_table_data, table_col[1], foreign_col[1])\n                    \n                    key_type_1 = 'PK'\n                    key_type_2 = 'PK'\n                    if (col_1_relationship == 'Contained') | (col_1_relationship == 'Overlap') | (col_1_relationship == 'Equal'):\n                        key_type_1 = self.set_col_keytype_to_FK(table, table_col[1], 'Metadata')\n                    if (col_2_relationship == 'Contained') | (col_2_relationship == 'Overlap') | (col_2_relationship == 'Equal'):\n                        key_type_2 = self.set_col_keytype_to_FK(foreign_table, foreign_col[1], 'Metadata')\n\n                    ref_left = TableRefDTO(table_name=table_col[0], column_name=table_col[1], key_type=key_type_1, \\\n                                           cardinality_type=cardinality_left, relationship_type=col_1_relationship)\n                    ref_right = TableRefDTO(table_name=foreign_col[0], column_name=foreign_col[1], key_type=key_type_2, \\\n                                            cardinality_type=cardinality_right, relationship_type=col_2_relationship)\n                    dependency = DependencyDTO(ref_left, ref_right, dependency_source='Metadata')\n\n                    dependencies_list.append(dependency)\n\n    #                 print(dependency.left.tableName+'.'+dependency.left.columnName+'_'+dependency.left.keyType+'_'+dependency.left.cardinalityType+'_'+\\\n    #                       dependency.left.relationshipType+'  '+\\\n    #                       dependency.right.tableName+'.'+dependency.right.columnName+'_'+dependency.right.keyType+'_'+\\\n    #                       dependency.right.cardinalityType+'_'+dependency.right.relationshipType+'  '+\\\n    #                       dependency.dependencySource)\n\n        return dependencies_list\n\n    def find_internal_foreign_keys(self, table, internal_dependencies_dict):\n        internal_dependencies_dict[table.name] = []\n        data = get_data(table, 0, to_sample=False)\n\n        pk_table, identifier_code_cols = self.get_pk_id_cols(table)\n\n        if len(pk_table) == 0:\n            return  \n\n        for pk_col in pk_table:\n            for id_col in identifier_code_cols:\n                if pk_col.rawType == id_col.rawType:\n                    col_1_relationship, col_2_relationship = self.get_relationship_type(data, data, pk_col.name, id_col.name)\n\n                    if col_2_relationship == 'Contained':      \n                        id_col.isFK = True\n                        id_col.FKsource = 'Internal'\n\n                        internal_dep = {'pk_col': pk_col, 'internal_fk_col':id_col}\n                        tables_dependencies_dict[table.name].append(internal_dep)                        \n                        print(internal_dep)\n\n    def update_internal_fk_to_external_table(self, table_name, pk_col, internal_fk_col, all_tables_dependencies):\n        for dep in all_tables_dependencies:\n            pk_ref = None\n            if (dep.left.tableName == table_name) & (dep.left.columnName == pk_col.name) & (dep.left.relationshipType == 'Contained'):\n                pk_ref = dep.right\n            elif (dep.right.tableName == table_name) & (dep.right.columnName == pk_col.name) & (dep.right.relationshipType == 'Contained'):\n                pk_ref = dep.left\n                \n            if pk_ref != None:\n                table = self.get_table_by_name(table_name)\n                table_data = get_data(table, 0, to_sample=False)\n                cardinality = self.get_cardinality(table_data, internal_fk_col.name)\n                ref = TableRefDTO(table_name=table_name, column_name=internal_fk_col.name, key_type='FK', \\\n                                  cardinality_type=cardinality, relationship_type='Contained')\n                \n                dependency = DependencyDTO(pk_ref, ref, dependency_source='Discovered')\n                all_tables_dependencies.append(dependency)\n\n                internal_fk_col.isFK = True\n                internal_fk_col.FKsource = 'Discovered'\n        \n    \n    \n    # Main function; extract dependencies between each possible combonation of two table from schema_discovery\n    def get_external_dependencies(self):\n        all_tables_dependencies = []\n        if self.metadata != None:\n            for table in self.schema_discovery.tables:\n                table_dep_from_metadata = self.add_table_dependencies_from_metadata(table)\n                all_tables_dependencies = all_tables_dependencies + table_dep_from_metadata\n\n        threads_list = []\n        internal_dependencies_dict = {}   \n        for table in self.schema_discovery.tables:\n            thread = threading.Thread(target=self.find_internal_foreign_keys, args=(table, internal_dependencies_dict,))\n            threads_list.append(thread)\n            thread.start() \n            \n        for thread in threads_list:\n            thread.join()\n            \n        print(internal_dependencies_dict)\n                \n        all_possible_table_combinations = [x for x in itertools.combinations(self.schema_discovery.tables, 2)]\n        threads_list = []\n        tables_dependencies_dict = {}\n        for idx, tables_tupple in enumerate(all_possible_table_combinations):\n            table_1 = tables_tupple[0]\n            table_2 = tables_tupple[1]\n            tables_dependencies_dict[idx] = []\n\n            thread = threading.Thread(target=self.find_foreign_keys, args=(table_1, table_2, tables_dependencies_dict, idx,))\n            threads_list.append(thread)\n            thread.start()\n\n        for thread in threads_list:\n            thread.join()          \n\n        for idx in range(len(all_possible_table_combinations)):\n            tables_dependencies = tables_dependencies_dict[idx]\n            all_tables_dependencies = all_tables_dependencies + tables_dependencies\n            \n        for table_name in internal_dependencies_dict:\n            for key_dict in internal_dependencies_dict[table_name]:\n                pk_col = key_dict['pk_col']\n                internal_fk_col = key_dict['internal_fk_col']\n                self.update_internal_fk_to_external_table(table_name, pk_col, internal_fk_col, all_tables_dependencies)\n\n        all_tables_dependencies = self.remove_duplicate_dependencies(all_tables_dependencies)\n        self.schema_discovery.dependencies = all_tables_dependencies\n        self.change_identifier_code_to_lable()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Create schema_discovery object using get_schema_discovery_from_DB function\n# User input\nread_from_db = True\nsample_percentage = 1\njdbcUsername = \njdbcPassword = \njdbcDriver = \njdbcHostname = \njdbcDatabase = \nport = \n\n\netl_object = ETL()\nschema_discovery, metadata = etl_object.create_schema_discovery_from_DB(jdbcUsername, jdbcPassword, jdbcDriver, jdbcHostname, jdbcDatabase, port, sample_percentage)\n\n# # Create schema_discovery object using get_schema_discovery_from_csv function\n# # User input\n# read_from_db = False\n# sample_percentage = 1\n# path = ''\n\n# etl_object = ETL()\n# schema_discovery, metadata = etl_object.create_schema_discovery_from_csv(path, sample_percentage)\n\n\n# Extract columns details for each table in schema_discovery using extract_columns_info function\ncolumns_info_extractor = ColumnsInfoExtractor(schema_discovery, metadata)\ncolumns_info_extractor.extract_columns_info()\n\n\n# Extract entities for each table in schema_discovery\nsample_size = 10000 # User unput\nentities_extractor = EntitiesExtractor(schema_discovery)\nentities_extractor.find_entities(sample_size)\n\n# Extract dependencies between each possible combination of two tables in schema_discovery using get_external_dependencies function\ndependencies_extractor = DependenciesExtractor(schema_discovery, metadata)\ndependencies_extractor.get_external_dependencies()\n\n# Save schema_discovery object\nschema_discovery_file_location = \"wasbs://hc-test-data-01@homecredittest01.blob.core.windows.net/hc-test-01/schema_discovery.json\" # User unput\nsave_schema_discovery(schema_discovery, schema_discovery_file_location)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">{&#39;credit_card_balance&#39;: [], &#39;bureau&#39;: [], &#39;application_test&#39;: [], &#39;bureau2&#39;: [], &#39;bureau_balance&#39;: [], &#39;sample_submission&#39;: [], &#39;application_train&#39;: [], &#39;application_test2&#39;: [], &#39;pos_cash_balance&#39;: [], &#39;installments_payments&#39;: [], &#39;previous_application&#39;: []}\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Extract dependencies between each possible combination of two tables in schema_discovery using get_external_dependencies function\ndependencies_extractor = DependenciesExtractor(schema_discovery, metadata)\ndependencies_extractor.get_external_dependencies()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["for table in schema_discovery.tables:\n  print(table.name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">credit_card_balance\nbureau\napplication_test\nbureau2\nbureau_balance\nsample_submission\napplication_train\napplication_test2\npos_cash_balance\ninstallments_payments\nprevious_application\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["for table in schema_discovery.tables:\n  if table.name == 'credit_card_balance':\n    break"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["dependencies_extractor = DependenciesExtractor(schema_discovery, metadata)\ndependencies_extractor.find_internal_foreign_keys(table, {})"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":14},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":15}],"metadata":{"name":"auto_discovery_spark","notebookId":537246058669394},"nbformat":4,"nbformat_minor":0}
